\documentclass[11pt,a4paper]{report}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[pdftex]{graphicx}

%\usepackage[nottoc,numbib]{tocbibind}
\usepackage{listings}
\usepackage{cite}
\usepackage{tocloft}
\usepackage{parskip}
\usepackage{float}
\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

%Preset style for all code listings is C
\lstset{
	language = C,
  basicstyle=\footnotesize,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  stepnumber=1,                   % the step between two line-numbers. wef it's 1, each line                                   % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code                    % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
      % sets if automatic breaks should only happen at whitespace
}


\setcounter{tocdepth}{3}
\addtocontents{toc}{\protect\vspace{10pt}}
\addcontentsline{toc}{chapter}{Bibliography}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\setlength{\parindent}{0cm}

\begin{document}
\input{./titlepage.tex}




\section*{Abstract}
With the onset of multi-core processors, writing software that takes advantage of such processors has become an essential part of modern software development. Unfortunately, writing concurrent programs is a difficult task and often produces many bugs which will not be found even after extensive testing. There are many scenarios such as data races and deadlocks that will go unnoticed until a system has gone to production, potentially leading to catastrophic results. To overcome this, many tools that trace concurrent execution have been created to debug concurrent software.

An existing tool KLEE-THREADS uses symbolic execution to trace concurrency bugs and output a thread schedule to show the conditions for the bug to arise.

Concurrency bugs are notoriously difficult to reproduce, since many bugs are dependent on a particular thread schedule that occurs only very rarely. The aim of this project has been to provide deterministic replay to concurrent software.

This tool takes a schedule generated by KLEE-THREADS and generates an executable in which the thread schedule will be followed. KLEE-THREADS achieves its schedule via symbolic execution. This tool provides a replay feature via function interposition. 

The tool works with the original Pthreads API and requires no extra-intervention on the part of the programmer. It incurs a minimal overhead and will follow any generated schedule accurately.

The Pthreads API is very large. As a result of this, we have been unable to implement a replay feature for the whole library. Instead, we have focused on three key aspects on the API: thread creation, thread termination, and mutex locking and unlocking. we have made a higly robust deterministic replay tool for these parts of the Pthreads API.

we have implemented the final tool by using function interposition. The tool will load my library and execute a binary that uses Pthreads while forcing the thread schedule it has been passed to be followed. wet operates in three modes. In its first mode, the tool will force the schedule and either hang or finish exeuction at the end of the schedule. In its second mode, the tool will follow the schedule to its end and then stop execution. In its third mode, the tool will follow the schedule until its end and then resume normal execution with no further intervention. This allows a schedule to be followed to a certain point and then program execution to be observed from that point onwards.


\newpage

\section*{Acknowledgements}


\newpage


\tableofcontents

\newpage
\clearpage

\chapter{Introduction}
Multi-core processors have moved from specialist computers and large-scale clusters to desktop and laptop computers and are becoming the norm in all areas of computing. The need to make use of multiple processors in software development has become essential and this had led to many new types of bugs emerging.

\section{Deterministic Replay}

The aim of this project has been to create a tool that provide deterministic replay to concurrent software written using the Pthreads API. Deterministic replay is used  to force a program to follow a certain path of execution. By reproducing a particular execution trace, a programmer is able to see an error that might not occur under normal conditions. Many bugs in concurrent software rely on a very specific thread schedule before they will occur. As a result of this, many such bugs will not appear even after thousands of executions. Deterministic replay will force a particular schedule to be followed and reproduce the circumstances that gave rise to the bug, thus allowing the programmer to quickly and accurately debug their software.

For deterministic replay to be successful, a thread schedule must be captured from a given program. Once the thread schedule has been captured, deterministic replay becomes possible by forcing the schedule to be followed. The tool presented in this paper uses an existing replay tool KLEETHREADS for detecting concurrency bugs in concurrent C programs that use the Pthreads API. The tool that is being presented takes a schedule from KLEETHREADS and then follows the path of execution as determined by the passed schedule.

The Pthreads API is a particularly large API and as a result of its size, we have not been able to implement deterministic replay for the whole API. Instead, we have focused on creating a robust tool to work with thread creation, thread joining, mutex locking and unlocking, and waiting on conditions signalling and notifying sleeping threads.


\section{Motivation}
Software bugs remain a major source of annoyance for software developers and with the advent of multi-core processors and the need to write concurrent software, new, more devastating bugs have arisen. These bugs are often difficult to track down and occur only under very specific conditions which might not arise even after extensive testing and executions. To ease the tracing of such bugs, many tools which provide execution traces by a number of techniques have been created.

One such tool KLEETHREADS, uses instrumentation to trace concurrency bugs in  concurrent C programs. While KLEETHREADS has proved excellent in tracing bugs, it does not support a replay feature to show programmers the bugs which it traces. The motivation behind this project has been to provide such a tool.

The main motivation for creating the tool is to add a deterministic replay feature to the KLEETHREADS tool which already provides a bug trace feature to the Pthreads API.

Programmers can often be sceptical about schedules they are shown which lead to bugs. Tools like \textsf{gdb} and other popular debuggers can often slow down execution and prevent bugs from occurring in the way that the schedule determined. By adding a deterministic replay feature to the KLEETHREADS tool, we will be able to show programmers the conditions under which their bug arises and demonstrate to them how to fix it.

By showing a programmer a bug quickly and allowing them to recreate the exact conditions under which the bug occurred, we will be able to speed up the debugging process significantly as through a combination of the schedules output by KLEETHREADS and the deterministic replay provided by this tool it is simple to detect, trace and fix bugs quickly and efficiently.

It is also desirable to create replay feature for a program without having to alter the original source code. This way the programmer can simply run the tool using their program and feed the tool a schedule. The tool will then follow this schedule faithfully to reproduce the conditions under which the bug occurred. By requiring no modification of the original executable, a user is able to use the tool with ease.

Some deterministic replay tools, however, achieve deterministic execution in non-deterministic programs but incur a large overhead. wedeally, a tool should incur as little overhead as possible. A tool that produces a large space and time overhead can be inconvenient to use and sometime traces may be so large that such tools will run out of memory before finishing the execution replay.


\section{Objectives}

The objectives for the project are as follows:

\begin{itemize}
\item
To provide a tool that takes schedules produced by KLEE-THREADS and accurately forces the thread schedules to be followed.
\item
To create three different modes of execution for the tool. One to simply follow a schedule and either hang or terminate execution depending on its current state. Two to follow a schedule either to its end or an arbitrary point and then resume normal execution so that results can be observed from a certain point.
\item
To create the tool such that it can be run with the original executables, so that no source-code level modifications are required and such that no re-linking is required.
\item
To create the tool while incurring as minimal runtime overhead as possible so that large schedules can be followed with minimal memory footprints.
\end{itemize} 

\section{Contributions}

The contributions for the project have been as follows:

\begin{itemize}
\item
A novel way of using function interception within the Linux kernel to interpose Pthread functions and force thread schedules to be followed.
\item
A deterministic replay tool that can be run with the original executable and the original functions in the Pthreads API.
\item
A deterministic replay tool that incurs a minimal additional overhead by way of function intercepting using the LD\_PRELOAD environment variable.
\end{itemize}

\section{Results}
The results for the project have been promising. The tool works successfully with a large number of thread schedules output by the KLEETHREADS tool and successfully executes with minimal overhead on the original executable. The tool only requires an executable that uses the Pthread API as well as a thread schedule for it to follow. 

The tool does not have to be used exclusively with KLEETHREADS as it will force execution on any schedule it is passed. Thus a user can create their own schedules for the tool to follow and observe the output. In this way, the tool is truly a deterministic replay tool for C programs and does not rely on any specific replay feature to function correctly.

The main limitations of the project have been in the size of the Pthreads API and the amount of time allocated to complete the project as well as some portability issues. As a result of the size of the Pthreads API, we have been unable to integrate the tool with every aspect of the APIAPI. Instead, we have focused on creating a robust tool for thread creation, thread termination, mutex locking and unlocking, and waiting on and signalling condition variables. 

There have also been some portability limitations with the tool. While the tool is guaranteed to work with the Linux kernel, the use of pthread\_tryjoin\_np(...) in thread termination calls limits its portabiliy across other POSweX compliant platforms.

\vspace{3em}
Below is a list of the Pthread functions we have implemented:

\input{APITable.tex}

\newpage



\section{Structure of this Report}
The rest of this report is structured as follows: Chapter 2 present background and related work, as well as some of the inspirations for the tool. Chapter 3 discusses the approach that was taken to the creation of the tool and the key concepts of the implementation. Chapter 4 discussed the development process and the problems encountered along the way. Chapter 5 discusses the testing process and the evaluation of the tool. Finally, Chapter 6 summarizes the document and concludes the achievements made by the project and possible future work.

\chapter{Background and Related Work}

Tools already exist that provide both record and replay features to concurrent software on a variety of platforms. This chapter introduces some of the existing record/replay tools and examines their effectiveness and how they have influenced the creation of this tool as well as giving an introduction to different concurrency problems and their solutions.

\section{Concurrency}
Concurrency can best be described as two or more things occurring at once, but not necessarily at the same time. wet differs from parallelism (although the too are often used interchangeably) since concurrency allows thread interleaving while parallelism generally refers to two or more threads running simultaneously. 

Concurrency has become a major problem for the modern software developer and many programs are expected to have at least a basic level of concurrency support. The time for developers to simply rely on increased CPU performance to speed up their programs has come to an end \cite{Sutter}.

Writing concurrent software produces bugs that would not usually occur in single-threaded applications. The most obvious example of such a bug is a data race. Data races occur when multiple threads of execution alter the value of a variable in shared memory at the same time. The result is indeterminate and in such cases the programmer cannot know what the value of the variable will be ahead of time. Data races can be resolved using simple concurrency techniques such as mutexes but these can require expertise to use properly and in large systems there may be so many data races that is is possible for a programmer to find them all.

Another bug common in concurrent software is a deadlock. Deadlocks are particularly nasty, since they often occur when a programmer has already taken measures to avoid concurrency bugs such as data races. An obvious path to deadlock is failing to unlock a mutex after obtaining it and forcing other threads to wait on that mutex. 

Concurrency bugs are difficult to reason about and there may be many possible paths of executions that can be followed in a given program state. As a result of this, even after extensive debugging and testing there may still be bug's in a concurrent system that go unnoticed for long periods of time and applications may even ship with undetected concurrency bugs which can lead to serious complications. 

In recent times the need to have sophisticated debugging tools for concurrent software has increased. wet is particularly useful to show a programmer their error rather than simply showing the error trace. 

\subsection{Types of Concurrency Bug}

Below are some common concurrency errors that can occur in concurrent software systems.

\subsubsection{Race Conditions}

A race condition occurs when two or more threads attempt to access a shared variable, without first obtaining a mutex lock. The reading of writing of a shared variable or state should be considered a critical section and should thus be mutually exclusive from other threads. In its critical section, a thread should not be interrupted to guarantee the integrity of its acces to the shared variable.

Data races can come in two forms. A critical data race occurs when the order in which the value of a variable or variables will effect the final outcome of the program. A non-critical data race occurs when no matter what order the variables are changed in the final state of the program remains the same.  


\subsubsection{Deadlock}

A deadlock is when two or more competing threads are waiting on each other with neither releasing their held resource, resulting in no progress being made in the system. A common cause of deadlock is a thread failing to unlock a mutex after finishing its critical section. 

The Coffman conditions \cite{Coff} listed below, are the required conditions for a deadlock to occur:
\begin{enumerate}
\item Mutual Exclusion: At least one resource must be non-shareable. Only one process can use the at resource at any given instant of time.
\item Hold and Wait or Resource Holding: A process of thread is currently holding at least one resource and requesting additional resourceswhich are being held by other processes.
\item No Preemption: The operating system must not de-allocate resources once they have been allocated; they must be released by the holding process voluntarily.
\item Circular Wait: A process of thread must be waiting for a resource which is being held by another process of thread, which in turn is waiting for the first process of thread to release the resource.
\end{enumerate}


\section{Bug Detection}

There exist some tools which have been created to detect concurrency bugs using a variety of techniques. we give a brief outline and evaluation of each of these tools below.

\subsection{KLEE-THREADS}

KLEE-THREADS uses dynamic symbolic execution to detect data races in concurrent C programs. wet is built on top of the KLEE symbolic virtual machine. KLEE is capable of automatically generating tests that achieve high coverage of complex programs \cite{KLEE}.KLEE uses the LLVM instruction set to map instructions to constraints without approximation and to execute the program within its framework. KLEE uses symbolic execution on a set of generated states to select and execute a wide variety of execution paths. KLEE handles non-determinism by modelling the native execution environment symbolically. This enables KLEE to consider multiple paths under a variety of situations, allowing it to model the non-deterministic state.

KLEE supports symbolic implementation of many POSIX functions so many programs using POSIX-compliant software can run in KLEE without having to recompile or instrument them.





\subsection{Helgrind}
Helgrind is a tool provided as part of the Valgrind tool suite for detecting synchronisation errors in C, C++ and Fortran programs that use the Pthreads API. wet focuses on three key areas of error detection: misuses of the Pthreads API, potential deadlocks, and data races.

Helgrind works in conjunction with Valgrind so executes a user's program within the Valgrind framework. wet intercepts Pthreads function calls to check for misuses of the Pthreads API and reports the errors back to the user. wet detects potential deadlocks by monitoring the order in which threads obtain locks and building a graph to represent this ordering. wef it detects a cycle in the graph it has built then it will raise a deadlock warning. 

In order to detect data races, Helgrind uses the Eraser algorithm \cite{Savage}. wet relies on the happens-before relation \cite{Lamport} to determine the expected ordering of data accesses. wef, through its monitoring of variable accesses, Helgrind detects that the accesses are ordered by the happens-before relation then nothing is raised. wef the accesses are not ordered by the happens-before relations, however, then Helgrind will signal a data race detection. The happens-before relation only provides partial ordering no total ordering -- that is that two variables are merely unordered with respect to each other.

Helgrind's main limitations are that at times it can output both false negatives -- undetected data races -- and false positives -- data races that may not actually be data races.  

\subsection{ThreadSanitizer}

ThreadSanitizer is part of the Google data-race-test tool suite and is a data race detection tool that seeks to eliminate the false negatives and false positives that are output by Helgrind and create a more efficient concurrent bug detection tool. The Linux and Mac versions of the tool are based on Valgrind \cite{valgrind} and the Windows version is base on PIN \cite{PIN}.

ThreadSanitizer runs as part of the Valgrind tool suite and uses a hybrid algorithm as well as a pure happens-before mode for efficient data race detection \cite{tsan}. 

ThreadSanitizer's hybrid algorithm works by observing a program's execution as a sequence of events. The key events are memory access and synchronisation events. A state machine is also used to maintain the history of its observed events \cite{tsan}. ThreadSanitizer's hybrid algorithm will report more false positives but will also detect more races.

Since ThreadSanitizer can operate in different modes, different results will be observed from each execution instance. In pure happens-before mode less data races will be detected and the results are less predictable but it will only report false positives if the program is custom-lock free. The advantage of ThreadSaniztier's pure happens-before mode is that ThreadSanitizer will report all locks involved in a race while a classical pure happens-before mode us unaware of locks and thus can't include them in its report\cite{tsan}.

ThreadSanitizer's main advantage is that is uses an existing tool, Valgrind, to provide better bug detection features than Helgrind, as well as being able to be run in multiple modes. When using ThreadSanitizer on our tool to detect a potential data race in the wrapper libary it proved more effective in finding the bug than Helgrind.



\section{Record/Replay Tools}

Record/replay in concurrent software debuggers is a feature that is provided to record a trace to a bug and then replay the conditions under which the bug occurred with the aim of reproducing the bug.

There are existing tools which provide both a record and replay feature to finding concurrency bugs. Below we list some of these tools and evaluate their advantages and disadvantages. Most of these tools focus on the Java programming language and target the Java Virtual Machine.

\subsection{CHESS}

\textsf{CHESS}  introduces the concept of concurrency scenario testing, in which a system designer considers interesting scenarios to test the system under \cite{chess}. In order to replay a given scenario CHESS uses model checking to systematically generate all interleavings within that scenario. A model checker captures all non-determinism within a system and then attempts to enumerate all of those possible choices. To capture non-determinism \textsf{CHESS} uses a simple abstraction for each asynchronous execution called a task. Any execution of a task results in a new state condition. To implement this abstraction layer, \textsf{CHESS} provides a  set of wrappers for the Win32 API.

\textsf{CHESS} controls the scheduling of tasks by by intstrumenting all of the functions in the concurrency API of the platform that create tasks and access synchronisation objects. wet does this to avoid perturbing the system more than absolutely necessary and in order to prevent performance limitations by having \textsf{CHESS} require its own modified version of the runtime platform. Only one thread is allowed to run at anytime to emulate the test on a single processore machine. The reason for this approach is to prevent threads that are running simultanaeously from creating data races that \textsf{CHESS} cannot control.

The main advantage of \textsf{CHESS} is its thoroughness. For each bug found by \textsf{CHESS}, it is able to consistently reproduce the error therefore making it signiificantly easier to show the error and thus to debug it. By allowing a system designer to test many scenarios, \textsf{CHESS} allows a certain amount of customisability in its debugging and thus increases its ease of use. The fact the \textsf{CHESS} requires no modified runtime platform to execute is also particularly useful as it minimises the overheads produced by using a recod/replay tool.  

The main disadvantage of \textsf{CHESS} is that it does not try to reproduce all sources of non-determinism resulting in an incomplete search path. \textsf{CHESS} will only search the passed thread schedule so if a bug depends on a particular series of inputs under certain conditions, it will not be found under a normal execution trace in \textsf{CHESS} 



\subsection{LEAP}
\textsf{LEAP's} general approach for tracing execution is to force each shared variable to record each thread access it has during a given execution \cite{Huang}. To achieve this it is required to precisely locate which variables can be considered shared. This is achieved by using a static field-based shared variable scheme to identify which variables are being accessed. 

\textsf{LEAP} relies on a transformer to provide instrumentation to an intermediate representation of Java bytecode. wet does this by instrumenting critical areas of the bytecode and inserting calls to the \textsf{LEAP} API so that correct record and replay can be recorder by the tool \cite{Huang}.

\textsf{LEAP} handles its record feature by invoking the \textsf{LEAP} API on each critical event it has identified to record the weD of the thread performing the access into the access vector. Once program execution has finished, LEAP will output a replay driver consisting of the thread creation order list and the access vector list \cite{Huang}.

\textsf{LEAP's} replay feature works by associating each thread in the schedule with a semaphore maintained in a global data structure to allow each thread to be suspended and resumed on demand \cite{Huang} rather than having to rely on Java's built in multi-threading tools which would not be suitable for \textsf{LEAP's} replay implementation.

\textsf{LEAP} is successful in that it provides a high-level of concurrent bug reproducibility while incurring a fairly minimal runtime cost. wets main disadvantage, however, is the need to insert instrumented API calls into an intermediary bytecode, since this means it cannot be used with pre-compiled Java programs. Also neither the record part nor the replay part of the tool can be used independently, which limits the tool's usability.

This tool takes a similar approach to \textsf{LEAP} in thread scheduling, since it associates all threads with a global pthread condtion variable so that threads can be put to sleep and woken on demand. This approach is advantageous in its simplicity since it requires no intervention with existing source code. 

\subsection{DejaVu}
\textsf{DejaVu} aims to provide a record/replay feature to the Java programming language by identifying a logical thread schedule which aims to make \textsf{DejaVu} more efficient \cite{DejaVu}.

A logical thread schedule consists of one or more thread schedules belonging to the same equivalence class. Schedules can be deemed equivalent if the physical thread schedule matches the ordering of shared variable accesses in a given execution \cite{DejaVu}. The information captured in a logical thread schedule is enough for \textsf{DejaVu} to reproduce the execution behaviour of a program during execution. 

\textsf{DejaVu} is a modified Java Virtual Machine that can provide deterministic replay to multi-threaded Java programs \cite{DejaVu}. This is a desirable feature since it means that the replay can be provided on existing Java bytecode with minimal overhead.

The main advantage of \textsf{DejaVu} is that it can be used with programs written in Java with no need for recompilation. The main disadvantage is that users wishing to \textsf{DejaVu} must use the virtual machine provided to debug their programs. 

\section{Other Approaches}

\subsection{Output Deterministic Replay}
\textsf{Output Deterministic Replay (ODR)} is a software only tool that targets a variety of software and provides a minimal overhead in its replay execution. wet is written in C and x86-Assembler and aims to be a lightweight middleware for Linux programs \cite{ODR}.  

The main difference between \textsf{ODR} and other replay tools is its approach to replication of the original file. \textsf{ODR} provides only a low-fidelity replay system, rather than the structure deterministic approach other tools provide \cite{ODR}.

The \textit{output-failure replay problem} is to ensure that any failures which occur in the original run of a program are visible in the replay of the program \cite{ODR}. This is a different approach to deterministic replay since it provides guarantees about the final program state rather than the execution path, allowing multiple paths to be explored in a given replay provided they lead to the same output. Tis can expose bugs that might occur under numerous circumstances.

To solve the output-failure replay problem \textsf{ODR} focuses on output determinism -- that is the output of the replay will match the output of the original executable. So reads and writes of shared variables can happen in a different interleaved order than in the original execution but their final values, or output, must be the same as at the end of the original program execution.

By relaxing its record model and focusing on output-determinism \textsf{ODR} provides a low-overhead recording system. wets main downside, however, is that it does not provide full deterministic replay for multiprocessor software systems, since it instead focused only on output-determinism. In all, \textsf{ODR} offers a unique and interesting new approach to providing a record/replay feature that incurs a minimal overhead.

\textsf{ODR's} approach, however, is not suitable to this tool. As the aim of the creation of this tool was to provide accurate replay of a given execution trace, simply determing the final output will be the same as in the original execution is not enough to provide formal deterministic replay for concurrent C programs, as KLEE-THREADS will output a schedule as far as a bug, thus resulting in schedules that are smaller than a full program execution which means there is no gaurantee of the final output of the program's execution.

\section{Function Interception Techniques}

Function interception can be used to intercept calls to functions at runtime and either interpose a different function in place of the one called or modify the behaviour of the original function. This project uses function interception to provide deterministic replay at runtime \cite{FunInt}.

Below, we go over some different techniques for intercepting function calls on a variety of platforms and evaluate their suitability for this project.

\subsection{Function Interposition in Linux}

In Linux, function interception can be achieved at runtime by using shared libraries and the LD\_PRELOAD variable. A user can specify a .so file as the file to be used first for looking up shared objects. Thus, at runtime, each call libraries will first be looked up in the library specified in the LD\_PRELOAD. wef the call is found, then the version in the .so provided will be used. wef there is no matching function call then the system will look for the function elsewhere.

wet is particularly useful for this project as it can be done in Linux with no recompling or relinking of binaries so the programmer can use function intercept to load a different library than the one called in their executable and observe different results. 

Functions provided by the dlfcn.h also make it simple to interpose function calls at runtime with functions provided in the .so file that is set in the LD\_PRELOAD variable. This project has made extensive use of the dlsym() function in particular. The dlsym(void *restrict handle, const char *restrict name) function returns a handle to the next object with the same name as specified in the name parameter. This handle can then be used to call the function named. This is expecially useful as it is possible to maintain the original functionality of the overridden function through this pointer.
  

\subsection{Detouring in Windows}

Function detouring intercepts function calls and re-write target functions at run-time. Detours \cite{detours} is a library for instrumenting Win32 functions dynamically at runtime. A function is intercepted and an unconditional call is inserted in its instructions to call the new function. A pointer to the original function is preserved via trampolining. 

Detouring is useful because they can be used on pre-compiled existing programs and are incredibly fast in their intercepting. we have not used the detouring technique, however, for this project as the Linux kernel provides the LD\_PRELOAD environment variable to intercept shared library calls at runtime.

Detouring is more focused towards extending existing binary software and adding additional functionality to such software. For the purposes of this project, however, simple function interception has proved adequate.

\subsection{Trampolining}

Function trampolining in C and C++ is typically moving from one code path to another. For example, in a C program you might want to call a C++ function. To do this, one might use trampolining by setting up the callback so that the C calling conventions will be converted into the C++ calling convention and allow the C++ method to be called.

Other uses for trampolining include, writing interrupt handlers in C rather Assembly by writing a short trampoline to convert the Assembly calling convention into the C calling convention. 

Function trampolining is particularly suited to this project but the main problem is using binary instrumentation to instrument the original executable and inserting the relevant ``trampoline." For this project it has been simple to use basic dynamic function interception as described above.



\chapter{Approach}
In my approach to creating the tool, we identified three key areas which needed to be resolved before implementation of the tool. These were: thread scheduling, the wrap() function, and the step\_and\_notify() function. In addition to these some consideration into the two modes of execution needed to be taken into account. 

In its current state the tool will run in three modes of execution as specified by a command line flag. In the first mode, the tool will simply follow the schedule to its end and either hang or continue execution dependent on the schedule length. In the second mode of execution, the tool will follow the schedule to its end and then terminate.  In the third mode of execution, the tool will force the schedule to be followed to its end but then will continue normal execution. all of these modes are discussed further below.


\section{Thread Scheduling}

In order to guarantee a particular thread schedule is followed it is necessary to provide a thread scheduler to schedule which threads should be run next. The role of the thread scheduler should be to put threads to sleep and wake them up on demand and explicitly specifiy which thread should be run next.

<<<<<<< .mine
This tool approaches thread scheduling in a simple manner. When threads are created they are assigned an instrumented ID which is matched with their pthread ID and placed into a C++ STL map container. Each time a synchronisation point occurs, a thread must call the wait function in order to check if it is eligible to be run. wef a thread's ID does match the current ID shown in the schedule then it will be required to wait on a global pthread condition variable.
=======
This tool approaches thread scheduling in a simple manner. When threads are created they are assigned an instrumented weD which is matched with their pthread weD and placed into a C++ STL map container. Each time a synchronisation point occurs, a thread must call the wait function in order to check if it is eligible to be run. wef a thread's weD does match the current weD shown in the schedule then it will be required to wait on a global pthread condition variable.
>>>>>>> .r38

<<<<<<< .mine
So that threads are not indefinitely put to sleep an additional function step\_and\_notify is used to move the schedule along to its next step and to broadcast to call of the sleeping threads so that they can check if they are eligible to be run.

The main limitation to this approach is that specific threads cannot be run on demand, since the broadcast will wake all waiting threads rather than one single thread. For example, if threads with instrumented IDs: 1,2,3 are all waiting on the global condition and we wished to wake thread 2 only, this would not be possible in the current implementation of the tool. This loss of fine-grained control, however, is made up for by the accurate following of a specific schedule that the tool allows. The tool will follow the schedule it has been passed faithfully. To observe different behaviour, or to choose a different thread to be run at a certain time, it is enough to simply pass a new schedule to the tool.

Since the tool will follow any schedule it is passed, a user can specify a schedule for it to follow to observe the results. This is particularly useful for scenario checking, since a system designer can write a specific schedule to lead to a certain scenario and then observe the behaviour. In this way, the tool can be used independently of KLEE-THREADS.


=======
So that threads are not indefinitely put to sleep an additional function step\_and\_notify is used to move the schedule along to its next step and to broadcast to call of the sleeping threads so that they can check if they are eligible to be run.

The main limitation to this approach is that specific threads cannot be run on demand, since the broadcast will wake all waiting threads rather than one single thread. For example, if threads with instrumented weDs: 1,2,3 are all waiting on the global condition and we wished to wake thread 2 only, this would not be possible in the current implementation of the tool. This loss of fine-grained control, however, is made up for by the accurate following of a specific schedule that the tool allows. The tool will follow the schedule it has been passed faithfully. To observe different behaviour, or to choose a different thread to be run at a certain time, it is enough to simply pass a new schedule to the tool.

Since the tool will follow any schedule it is passed, a user can specify a schedule for it to follow to observe the results. This is particularly useful for scenario checking, since a system designer can write a specific schedule to lead to a certain scenario and then observe the behaviour. In this way, the tool can be used independently of KLEE-THREADS.


>>>>>>> .r38
\section{Wait}
The wait() function is crucial to ensuring that threads wait until they are eligible to be run. wet is implemented by using a global pthread\_cond\_t to force all threads to wait on the schedule. By taking this approach, threads can be effectively put to sleep and woken up on demand. 

<<<<<<< .mine
A thread will be required to call the wait() function at each synchronisation point, so that correct synchronisation of all active threads can be achieved. Once a thread has entered a wait state, it will wait until its instrumented ID matches the current ID in the schedule. This approach means that only the scheduled thread will be allowed to run next and all other threads will be required to wait and in this way thread can be put to sleep and woken up on demand each time they call a function that requires synchronisation. 
=======
A thread will be required to call the wait() function at each synchronisation point, so that correct synchronisation of all active threads can be achieved. Once a thread has entered a wait state, it will wait until its instrumented weD matches the current weD in the schedule. This approach means that only the scheduled thread will be allowed to run next and all other threads will be required to wait and in this way thread can be put to sleep and woken up on demand each time they call a function that requires synchronisation. 
>>>>>>> .r38

we also make use of the original Pthreads API, specificaly the Pthread condition functions which means no source code instrumentation is required.


Below is the source code for the wait() function:

\lstinputlisting[float=h!,caption=The wait function]{wait.c}



\section{Step and Notify}
The step\_and\_notfiy() function is used to increment the current schedule step and to wake threads which are currently sleeping in the wait() function. Step is a global variable used to monitor the progress of the excutable and to index the schedule. Each time a thread enters and leaves a synchronisation function a call to step\_and\_notify() is made. The first call wakes other threads so that if the callee is not eligible to be run, the schedule thread can begin execution. The second call will once again wake the sleeping threads in order to allow the next scheduled thread to resume execution. Each call to the step\_and\_notify() increments the global step variable and moves the schedule along.

Like the wait() function step\_and\_notify() makes use of an existing Pthread API function -- specifically pthread\_cond\_broadcast(). The function must call this to ensure that threads waiting in the wait() function are woken so that the schedule can be checked and the eligible thread can be run. The tool has no current implementation to intercept the pthread\_cond\_broadcast() function so there is no need to call it through a handle returned by dlsym() and the original API function can be used without issue.

To ensure the safe protection of the global step variable, each thread that enters ste\_and\_notify() must lock a mutex before performing any write access of the step variable. Without this measure, it would be impossible to follow a schedule in its entirety without getting skewed values for step.

<<<<<<< .mine
\vspace*{0.3em}

Below is the source code for the step\_and\_notify() function:

\lstinputlisting[float=h!,caption=The step\_and\_notify function]{stepandnotify.c}

\section{Modes of Execution}

When approaching the development of the tool, it seemed most appropriate to offer three modes of execution so that users could specify the results they might want to observe 

\subsection{Default Execution Mode}

In its standard mode of execution, the tool will follow a schedule to its end and then either hang or terminate depending on the length and values within the schedule. Used in conjunction with KLEE-THREADS, in this mode the tool will simply cease execution and hang at the end of the schedule. The purpose of this execution mode is to show the user their bug occurring under the very specific conditions that it might occur in the first place. 

As an example execution run, consider the following:
\begin{itemize}
\item The tool is fed the following schedule which leads to a data race: {0,0,0,1,1,2}.
\item The tool will run thread 0 three time and then switch to thread 1.
\item Thread 1 will be executed twice before thread 2 is executed.
\item At this point the tool will hang.
\end{itemize}

In this example, the exact conditions under which the data race occurred have been reproduce by the tool. Thus, a programmer can observe the output of the thread schedule being followed and recreate the condition under which the data race occurred. wet is important to note that in a data race scenario as described above the tool, will hang as a result of the schedule end being reached not as result of a data race occurring. 

\subsection{Follow-to-end and Terminate}

In this mode of execution the tool will follow a schedule until its end and then terminate. The aim is to show a user the first bug and reproduce the conditions under which the bug arose. wet is important to note that in this mode the tool will only terminate if the schedule has led to a concurrency bug which will allow termination. 

For example, a schedule that leads to a deadlock will be followed to that deadlock and then hang. This is because the checking for the end of the schedule is not pre-empted and the program will only execute once the end of the schedule has been reached and if a schedule leads to a deadlock then this deadlock will occur.

To achieve this follow and terminate technique. The script that starts the tool sets an environment variable which is read by the .so library at the start of the program. wef the FOLLOW\_AND\_TERMINATE environment variable has been set then this flag will be raised and each time the wait() function is invoked, the tool will check whether or not the end of the schedule has been reached. wef it has then the program will execute. The downside to this approach occurs in the checking of global step to determine if the schedule has come to an end, since multiple threads will call wait() at the same time thus creating a reader's/writer's problem. This is trivial to solve using mutexes, however, and maintains the minimal overhead the program incurs.

\subsection{Follow-to-end and Resume Normal Execution}

When running in this execution mode, the tool will follow the schedule it is passed until its end and then resume normal execution. This allows a particular path through a program's execution to be followed and then the remaining execution of that program to be observed. 

The user's program will not, however, be executing the original Pthreads functions that have been intercepted by the tool. This is because once the LD\_PRELOAD variable has been set the loader will only look in the location set in the variable for shared library objects. Thus all calls to Pthread functions supported by the tool will still be intercepted until the program has finished its execution. To overcome this limitation, the script that launches the tool will set an environment variable FOLLOW\_TO\_END which will be checked by the tool on the first call to pthread\_create(). The tool will set the relevant flag and once the end of the schedule has been reached, all further calls to intercepted Pthread functions will skip the wait() and steps\_and\_notify() functions and instead simply call the orignal API functions through the handles returned by calls to dlsym(). 

The function which determines if we have reached the end of the schedule and can resume normal execution is shown below:

\lstinputlisting[float=h!, caption=no\_follow() function] {nofollow.c}

This approach means that the original executable can continue to run as normal, while incurring a minimal overhead in its call to the intercepted functions.

One problem we encountered when implementing this mode was determining if threads should call the normal versions of the Pthreads API functions of the overridden one. There were several cases in which we found that after the no\_follow() function was returning true, threads which had been running in parallel were already being executed with the overridden function because they had already progressed past the points which determined which function to call.

As an example of this problem consider the small code snippet below:

\lstinputlisting[float=h!, caption=Mutex locking when FOLLOW\_TO\_END set]{badendmutex.c}

Consider two threads: A and B.

In this example, we can see that it might be possible for thread A to enter the pthread\_mutex\_lock function before the end of the schedule has been reached so it will enter the overridden branch of the code and call pthread\_trylock. While it is in this branch, thread B could cause the end of the schedule to be reached and the no\_follow function would start returning 1. Thus thread A would be calling pthread\_trylock rather than the original pthread\_mutex\_lock and so not executing as normal. In the case that there was a locked mutex that thread B was trying to obtain, B would exit this function normally as if it had obtained the mutex. This would of course, avoid any deadlocks or data races that might occur at the end of a particular schedule.

To fix this problem, we inserted calls to no\_follow() before returning from our overridden function calls, so that in an event similar to above, the original pthread functions would still be called. While this added a small extra overhead to the calls to the overridden functions, it elminated the possibility that threads would still be calling our overridden function when they were meant to be calling the normal functions.

\subsection{Record Mode}

The tool's record mode's purpose is to detect a given deadlock and then output the schedule that was followed leading to this deadlock. 

In order to build a schedule independent of the tool's execution mode, we inserted a call to a C++ function that dynamically builds a schedule into each call to step\_and\_notify(). Since step\_and\_notify() is called as every synchronisation point, we were able to trace the order in which threads were executing in any given execution. After the monitor thread detects a deadlock, it will output the schedule the tool followed to a file so that this file can then be fed to the tool and the execution replayed.

The tool's minimalist replay mode was implemented using a simple monitor thread. On the first call to pthread\_create() in any program, the tool will create and initialise a thread to monitor the progress of thread execution. The monitor thread's aim is to check for satisfactory progress and, if satisfactory progress has not been made, to terminate program execution and print the schedule that the tool followed to reach the deadlocked state. 

My initial approach to the monitor thread is shown below:

\lstinputlisting[float=h!, caption=Initial Monitor Thread]{monitorbad.c}


The problem with this initial approach is that it will only check for deadlocks if the tool is passed an initial schedule. The other issue with this implementation is that the comparison between last\_step and the global step is almost redundant, since one we would assume that the step and last\_step are being continuously updated, therefore they will never be equal, so this comparison is rather pointless. To create a better monitor thread it was necessary to determine if the global step variable was still being implemented. 

We did this in the approach shown below:

\lstinputlisting[float=h!, caption=Modified Monitor Thread]{monitorgood.c}

As you can see this monitor will work independently of a schedule and will work with the tool if no schedule is passed, since the global step is incremented each time step\_and\_notify is called and the tool builds the followed schedule each time step\_and\_notify is called.

The record mode can in fact be used to trace any execution schedule in a concurrent C program. This is in keeping with the tool's independence, since it means that a user can obtain a trace of execution should they wish to.

\subsection{Modified wait()} 

The modified version of the wait() function to account for the different execution modes is shown below:

\lstinputlisting[float=h!,caption=The modified wait function]{fullwait.c}










=======
\vspace*{0.3em}

Below is the source code for the step\_and\_notify() function:

\lstinputlisting[float=h!,caption=The step\_and\_notify function]{stepandnotify.c}

\section{Modes of Execution}

When approaching the development of the tool, it seemed most appropriate to offer three modes of execution so that users could specify the results they might want to observe 

\subsection{Default Execution Mode}

In its standard mode of execution, the tool will follow a schedule to its end and then either hang or terminate depending on the length and values within the schedule. Used in conjunction with KLEE-THREADS, in this mode the tool will simply cease execution and hang at the end of the schedule. The purpose of this execution mode is to show the user their bug occurring under the very specific conditions that it might occur in the first place. 

As an example execution run, consider the following:
\begin{itemize}
\item The tool is fed the following schedule which leads to a data race: {0,0,0,1,1,2}.
\item The tool will run thread 0 three time and then switch to thread 1.
\item Thread 1 will be executed twice before thread 2 is executed.
\item At this point the tool will hang.
\end{itemize}

In this example, the exact conditions under which the data race occurred have been reproduce by the tool. Thus, a programmer can observe the output of the thread schedule being followed and recreate the condition under which the data race occurred. wet is important to note that in a data race scenario as described above the tool, will hang as a result of the schedule end being reached not as result of a data race occurring. 

\subsection{Follow-to-end and Terminate}

In this mode of execution the tool will follow a schedule until its end and then terminate. The aim is to show a user the first bug and reproduce the conditions under which the bug arose. wet is important to note that in this mode the tool will only terminate if the schedule has led to a concurrency bug which will allow termination. 

For example, a schedule that leads to a deadlock will be followed to that deadlock and then hang. This is because the checking for the end of the schedule is not pre-empted and the program will only execute once the end of the schedule has been reached and if a schedule leads to a deadlock then this deadlock will occur.

To achieve this follow and terminate technique. The script that starts the tool sets an environment variable which is read by the .so library at the start of the program. wef the FOLLOW\_AND\_TERMINATE environment variable has been set then this flag will be raised and each time the wait() function is invoked, the tool will check whether or not the end of the schedule has been reached. wef it has then the program will execute. The downside to this approach occurs in the checking of global step to determine if the schedule has come to an end, since multiple threads will call wait() at the same time thus creating a reader's/writer's problem. This is trivial to solve using mutexes, however, and maintains the minimal overhead the program incurs.

\subsection{Follow-to-end and Resume Normal Execution}

When running in this execution mode, the tool will follow the schedule it is passed until its end and then resume normal execution. This allows a particular path through a program's execution to be followed and then the remaining execution of that program to be observed. 

The user's program will not, however, be executing the original Pthreads functions that have been intercepted by the tool. This is because once the LD\_PRELOAD variable has been set the loader will only look in the location set in the variable for shared library objects. Thus all calls to Pthread functions supported by the tool will still be intercepted until the program has finished its execution. To overcome this limitation, the script that launches the tool will set an environment variable FOLLOW\_TO\_END which will be checked by the tool on the first call to pthread\_create(). The tool will set the relevant flag and once the end of the schedule has been reached, all further calls to intercepted Pthread functions will skip the wait() and steps\_and\_notify() functions and instead simply call the orignal API functions through the handles returned by calls to dlsym(). 

The function which determines if we have reached the end of the schedule and can resume normal execution is shown below:

\lstinputlisting[float=h!, caption=no\_follow() function] {nofollow.c}

This approach means that the original executable can continue to run as normal, while incurring a minimal overhead in its call to the intercepted functions.

One problem we encountered when implementing this mode was determining if threads should call the normal versions of the Pthreads API functions of the overridden one. There were several cases in which we found that after the no\_follow() function was returning true, threads which had been running in parallel were already being executed with the overridden function because they had already progressed past the points which determined which function to call.

As an example of this problem consider the small code snippet below:

\lstinputlisting[float=h!, caption=Mutex locking when FOLLOW\_TO\_END set]{badendmutex.c}

Consider two thread A and B.

In this example, we can see that it might be possible for thread A to enter the pthread\_mutex\_lock function before the end of the schedule has been reached so it will enter the overridden branch of the code and call pthread\_trylock. While it is in this branch, thread B could cause the end of the schedule to be reached and the no\_follow function would start returning 1. Thus thread A would be calling pthread\_trylock rather than the original pthread\_mutex\_lock and so not executing as normal. In the case that there was a locked mutex that thread B was trying to obtain, B would exit this function normally as if it had obtained the mutex. This would of course, avoid any deadlocks or data races that might occur at the end of a particular schedule.

To fix this problem, we inserted calls to no\_follow() before returning from our overridden function calls, so that in an event similar to above, the original pthread functions would still be called. While this added a small extra overhead to the calls to the overridden functions, it elminated the possibility that threads would still be calling our overridden function when they were meant to be calling the normal functions.

\subsection{Modified wait()} 

The modified version of the wait() function to account for the different execution modes is shown below:

\lstinputlisting[float=h!,caption=The modified wait function]{fullwait.c}


>>>>>>> .r38
\chapter{Development}

<<<<<<< .mine
\section{Thread ID creation and Assignment}

When a thread is created using pthread\_create() an ID is assigned to the pthread. This ID is usually a hexadecimal number that is determined by the operating system. Since KLEE-THREADS aims to work with any concurrent C programs and it cannot know ahead of time what thread ID the operating system will assign to threads, it outputs its schedules with thread IDs starting at 0. An example output might be (0,1,2,3). Thread 0 will always represent the main thread and each thread from then on will be assigned an ID. 

Since the tool receives a schedule in this format, it was necessary to add instrumented ids to the threads as they were created so that a schedule could be followed without any modification. 

To achieve this, each time a thread is about to be run its  instrumented ID along with its pthread ID is inserted into a C++ STL map which holds a mapping of instrumented IDs to pthread IDs. Each time a thread enters a section in which its ID is required to continue, a call to pthread\_self() is made to obtain the pthread ID. This pthread ID can then be used to index the map and obtain the instrumented ID of the calling thread. Since code marked with ``extern C" will prevent the C++ compiler from mangling the names, a C program is able to call the C++ functions as if they were C functions. Doing this, enabled me to use C++ libraries and tools with the libraries written in C. 

An example Thread Map might look as follows:

\input{SampleMap.tex}

Using an STL map enables fast look-up of thread IDs and also means that nothing needs to be passed to the wait() and step\_and\_notfiy() functions to verify the thread being called. 

Two functions achieve the above functionality. The first  put\_instrumented\_id(pthread\_t p, int current\_id) will create a mapping between the pthread\_t p and the current\_id. In this case, current\_id is a global variable, incremented on each call to pthread\_create() to hold the id of the next thread to be created. To prevent data races,  a thread must lock the global current\_id before incrementing it.

The second function get\_instrumented\_id(pthread\_t p) will look up and return the instrumented ID of the pthread p in the STL map defined in the thread\_map.cpp file. The returned ID can then be used by the tool to check the schedule and determine if the calling thread is eligible to be run.

\clearpage
Below is the source code for the Thread Map:
\vspace{1em}
\lstinputlisting[float=h!,caption=The Thread Map]{thread_map.cpp}


=======
\section{Thread ID creation and Assignment}

When a thread is created using pthread\_create() an weD is assigned to the pthread. This ID is usually a hexadecimal number that is determined by the operating system. Since KLEE-THREADS aims to work with any concurrent C programs and it cannot know ahead of time what thread ID the operating system will assign to threads, it outputs its schedules with thread IDs starting at 0. An example output might be (0,1,2,3). Thread 0 will always represent the main thread and each thread from then on will be assigned an ID. 

Since the tool receives a schedule in this format, it was necessary to add instrumented ids to the threads as they were created so that a schedule could be followed without any modification. 

To achieve this, each time a thread is about to be run its  instrumented weD along with its pthread weD is inserted into a C++ STL map which holds a mapping of instrumented weDs to pthread weDs. Each time a thread enters a section in which its weD is required to continue, a call to pthread\_self() is made to obtain the pthread weD. This pthread weD can then be used to index the map and obtain the instrumented weD of the calling thread. Since code marked with ``extern C" will prevent the C++ compiler from mangling the names, a C program is able to call the C++ functions as if they were C functions. Doing this, enabled me to use C++ libraries and tools with the libraries written in C. 

An example Thread Map might look as follows:

\input{SampleMap.tex}

Using an STL map enables fast look-up of thread weDs and also means that nothing needs to be passed to the wait() and step\_and\_notfiy() functions to verify the thread being called. 

Two functions achieve the above functionality. The first  put\_instrumented\_id(pthread\_t p, int current\_id) will create a mapping between the pthread\_t p and the current\_id. In this case, current\_id is a global variable, incremented on each call to pthread\_create() to hold the id of the next thread to be created. To prevent data races,  a thread must lock the global current\_id before incrementing it.

The second function get\_instrumented\_id(pthread\_t p) will look up and return the instrumented weD of the pthread p in the STL map defined in the thread\_map.cpp file. The returned weD can then be used by the tool to check the schedule and determine if the calling thread is eligible to be run.

\clearpage
Below is the source code for the Thread Map:
\vspace{1em}
\lstinputlisting[float=h!,caption=The Thread Map]{thread_map.cpp}


>>>>>>> .r38
\section{Wrapper Library}

Before implementing the tool using function intercepting, we decided to write a wrapper class to use with my own Pthread programs. The purpose of creating such a wrapper was to come to terms with the Pthreads API but also to make sure that the key parts of the tool functioned in this environment first before moving on to the more complex function interception techniques. 

This part of the development process was purely for experimental purposes and to understand the processes involved in designing the tool. The obvious downside to using the wrapper library exclusively would have been the need for the program to be recompiled with the wrapper library included and the program to explicitly call the overridden functions.

To help with reasoning about how schedules should be followed, we wrote programs in which several threads accessed and altered a global variable without attempting to lock a mutex. The aim here was to write schedules that would induce a particular value for the global variable at the end of the program's execution. 

In the following program:

\lstinputlisting{induce.c}

The schedule: (0,0,0,0,0,1,3,4,2,0,0,0,0,0) will guarantee that the final value of GLOB will be 3. A minor change to this schedule will force the GLOB to be 1,2, or 3. The purpose of this exercise was to ensure that the tool could faithfully follow certain schedules and enforce a particular value for the global variable at the end.

By opting to write a wrapper library to see how the tool would work before implementing any dynamic interception, we were able to get to grips with how the threads could be put to sleep and woken on demand and also how thread scheduling should work. Once the wrapper was at a workable state and we was satisfied with the way it handled shcedules, then we could move on to implementing the tool via function intercepting.


\subsection{Problems}

<<<<<<< .mine
The main problem we encountered when implementing the wrapper library was in inserting the correct thread ID into the Thread Map. 

\clearpage
The problem occurred in thread creation. The initial intercept\_pthread\_create looked as follows:
\lstinputlisting[float=h!, caption=intercept\_pthread\_create]{wrapper.c}

The problem in this implementation occurs in line 25. While a thread is inserting the thread ID into the thread map, it may be that another thread is calling intercept\_pthread\_create and thus a data race is occurring. Initially this problem was hard to track down. in order to find it, we made use of the ThreadSanitizer tool, which showed me the data race. 

Using an existing concurrency bug detection tool during the development of the wrapper library was particularly useful to the overall progression of the project because it gave me a clear idea of how concurrency bugs can be traced and alerted me to some of the problems implicity in writing any kind of concurrent software. we also ran Helgrind on the tool to try and determine the cause of the problem. While both tools found the bug, Helgrind reported more false positives than ThreadSanitizer. 

This bug was easily fixed by placing the call to put\_instrumented\_id in the run\_thread function to guarantee that the calling thread had their ID place in the Thread Map rather than another thread accessing the map at the same time. As a result of the semantics of STL maps, the keys were kept unique so concurrent writes would not result in existing threads having their IDs overwritten.

The new run\_thread function is shown below:
\lstinputlisting[float=h!, caption= run\_thread with data race removed]{runrace.c}


After fixing the bug the only race conditions that either tool found were concurrent reads of the Thread Map, which is acceptable since there might be occasions on execution when more than on thread is required to read the thread map to obtain its instrumented ID in order to determine its eligibility to be run.

=======
The main problem we encountered when implementing the wrapper library was in inserting the correct thread weD into the Thread Map. 

\clearpage
The problem occurred in thread creation. The initial intercept\_pthread\_create looked as follows:
\lstinputlisting[float=h!, caption=intercept\_pthread\_create]{wrapper.c}

The problem in this implementation occurs in line 25. While a thread is inserting the thread weD into the thread map, it may be that another thread is calling intercept\_pthread\_create and thus a data race is occurring. Initially this problem was hard to track down. in order to find it, we made use of the ThreadSanitizer tool, which showed me the data race. 

Using an existing concurrency bug detection tool during the development of the wrapper library was particularly useful to the overall progression of the project because it gave me a clear idea of how concurrency bugs can be traced and alerted me to some of the problems implicity in writing any kind of concurrent software. we also ran Helgrind on the tool to try and determine the cause of the problem. While both tools found the bug, Helgrind reported more false positives than ThreadSanitizer. 

This bug was easily fixed by placing the call to put\_instrumented\_id in the run\_thread function to guarantee that the calling thread had their ID place in the Thread Map rather than another thread accessing the map at the same time. As a result of the semantics of STL maps, the keys were kept unique so concurrent writes would not result in existing threads having their IDs overwritten.

The new run\_thread function is shown below:
\lstinputlisting[float=h!, caption= run\_thread with data race removed]{runrace.c}


After fixing the bug the only race conditions that either tool found were concurrent reads of the Thread Map, which is acceptable since there might be occasions on execution when more than on thread is required to read the thread map to obtain its instrumented weD in order to determine its eligibility to be run.

>>>>>>> .r38
\section{Function Interposition}

\subsection{Recursive Function Calls}

Recursive function calls was one of the main problems we face when implementing function interception. The reason for this was that the library which the tool uses to intercept function calls makes heavy use of the existing Pthreads API. For example, when a thread makes a call to lock a mutex, eventually that mutex must be locked, or else execution of the program will become indeterminate. 

Another problem is that the wait() function, which forces thread to wait their turn in the schedule makes use of both mutexes and condition variables. Any thread, therefore, that makes a call to wait() would inadvertently call the the overridden pthread\_mutex\_lock and pthread\_mutex\_unlock functions resulting in a recursive function call and potentially the programm crashing of reaching a deadlock.

To overcome this problem, we made use of the dlsym(void* restrict handle, const char* restrict name) function, which looks up the next object with the name specified in the name parameter and returns a reference to the function specified. 

Using these features, we have essentially renamed the real Pthreads API functions so that we can then call those functions from within the functions that we have overridden. This enables me to run a thread and lock and unlock mutexes without having the problem of recursive function calls. So when a user's program makes a call to the pthread\_create, my tool will intercept this and then call the real system-implemented version of pthread\_create to run the functions specified by the user in their function call.

\subsection{Dynamic Interception}

Once the tool was working robustly with the wrapper library, we moved on to implementing it using dynamic function intercepting. As described previously, by setting the LD\_PRELOAD variable it is possible to intercept calls to shared libaries at runtime. With this approach there is no need for the program to be recompiled or relinked to make use of the tool, as all calls to the Pthreads API within the program will be intercepted by the specified shared library. This is a major advantage to the programmer since it requires no additional effort on their part and means that the tool can be used with all existing programs that use the Pthreads API. 

One of the main challenges was passing the tool the shedule to follow. So that the tool has access to the schedule, when the script is run to set up the tool various environment variables must be set which can then be accessed by the tool to read the schedule and determine what mode of execution it should be run in. The schedules is held in the custom defined KLEE\_SCHED variable. The tool will look for the file set in this variable and attemp to read a schedule from it. wef the environment variable is not set, or no schedule is found or if the schedule is invalid, then the tool will exit. Once the schedule has been read into the tool, it can be followed as normal.

The obvious problem with the above approach in the setting of excessive environment variables while the tool is executing. Failing to unset these environment variables after execution might cause problems if other programs need to use them. This is easily resolved, however, as long as the relevant environment variables are unset after program termination.

\subsubsection{Problems}
<<<<<<< .mine
=======

The main problem we encountered when using dynamic interception to implement the tool was with the LD\_PRELOAD environment variable. Unless otherwise specified, setting the LD\_PRELOAD variable will be operating system wide. So any calls by any programs to the Pthreads API will result in those calls being intercepted. Needless to say this can be disastrous for a user if not handled correctly, since this can cause the whole operating system to crash. To remedy this problem, we wrote the script to set the LD\_PRELOAD variable for only the executable which is is passed and to unset the LD\_PRELOAD variable as soon as execution finisheds. This prevent any unneccessary function interception and contains the intercepting to only the program the user wishes to test. 

Another issue we found when using the LD\_PRELOAD was an issues with a program's permissions. In order to prevent a malicious user from intercepting system calls, a program must have the relevant permission to link and load a dynamic library. The GNU loader will check a program's permission on loading and determine if it is setuid or setgid. The loader will generally assume that a program does not have these permission and will thus greatly limit its permissions on loading. While this is not problematic when running programs from within the user's own profile, running programs on a remote server via ssh or on any machine where the owner of the program does not have priveleged acces will result in a segmentation fault on loadtime.  

There also a problem when using dlsym() to obtain a handle to function calls. In particular the problem arose when implementing pthread\_cond\_wait(). The dlsym() function makes an unversioned lookup and by default this lookup will match the oldest symbol in the dynamic shared library. wef any new features are added to a libary in the future then the dlsym() will still call the older version. Failing to use a versioned dlsym() call resulted in the older version of pthread\_cond\_wait being returned which returned a reference to a different pthread\_cond variable. wef the the initial pthread\_cond if located at 0x600d80, for example, then using the unversioned dlsym() function will result in the pthread\_cond variable begin returned at 0x7ffff00008c0. In this scenario, however, the main thread will still assume that the pthread\_cond variable is located at 0x600d80 and so will wait on this variable and receive no signal. To resolve this problem, it was neccessary to use a versioned dlysym() call to return the GLweBC\_2.3.2 version of the pthread\_cond\_wait() function.

\subsection{Pthreads API}

This project intercept calls made to thread creation, thread joining, mutex locking and unlocking and wait on condition variables. The various implementation details for each of these functions are discussed below.

\subsubsection{pthread\_create() and run\_thread()}

The source code for pthread\_create() using function intercepting is shown below:
\lstinputlisting[float=h!, caption=Intercepting pthread\_create]{create.c}

On the first call to pthread\_create() the tool will look for the location of the schedule in an environment variable and read the schedule file using this as its schedule. The tool will also check for the relevant flags and set these flags accordingly.

Each time a thread is created a params struct is instantiated to hold the relevant arguments for a thread. 

The params struct is shown below:
\lstinputlisting[float=h!, caption=The params struct]{params.c}

Params holds a function pointer to the function to be run in the created pthread, the instrumented weD of the thread and any arguments that need to be passed to the thread function.

Once a thread's parameters have been set up, a call to the real pthread\_create() function is made through the function pointer real\_create. This newly created thread is then told to run the run\_thread() function and is passed the params struct as its arguments. At this point in the execution, the params struct is fully instantiated so the original callee's function can be run through the run\_thread() function.

The run\_thread() function is shown below:
\lstinputlisting[float=h!, caption=The run\_thread function]{runthread.c}


The first thing the run\_thread() function does is insert the calling thread's weD into the Thread Map. The reason this is done in this function rather than on the call to pthread\_create() is because a call to create a pthread must be done by an existing thread and thus calls to pthread\_self() will return the pthread weD of the thread which called the create function not the weD of the thread which we are intending to create.

For example, consider the following simple scenario:
\begin{itemize}
\item The main thread with instrumented weD call pthread\_create()
\item pthread\_self() is called to obtain the weD of the thread to place in the Thread Map
\item Because the call to pthread\_self() was made by the main thread its pthread weD will be returned.
\item Thus there will be two duplicate entries for the pthread weD for the main thread with different instrumented weDs.
\end{itemize} 

For this reason, it far more desirable to insert the ID in the run\_thread() function.

Before a thread can be run it must wait on the schedule. Once it is eligible to be run the function passed in the params struct will be executed. A function might make other calls to Pthread API functions and these are handled accordingly. After a thread has finished its execution. wet must increase the schedule step and notify other waiting threads so that they can run.

By creating another thread to run the function that was passed to the initial call to pthread\_create(), we am able to force threads to follow the schedule. Without this approach, threads would have to be run within the call to pthread\_create() and this would cause threads to be forced to wait on each thread creation call, resulting in no proper thread scheduling. The thread created for run\_thread can be viewed as a worker threads, as it is designated to handle the thread scheduling without any other threads having to wait on its completion.

\subsubsection{pthread\_join()}
Below is the implementation for pthread\_join():

\lstinputlisting[float=h!, caption=wemplementation for pthread\_join]{pthreadjoin.c}

When a call to pthread\_join(pthread\_t, void** retval) is made, the calling thread will wait for the thread specified in pthread\_t to terminate and any return values are stored in the retval. 

When a call to pthread\_join is made. The calling thread must increase the global step to move the schedule along and notify waiting threads. This is to ensure that other threads can run before the real call to pthread\_join is made, blocking the calling thread. The calling thread must then wait until it is scheduled to run. After waiting the calling thread calls the pthread\_tryjoin function on the weD of the threads it is trying to join. 

The pthread\_tryjoin\_np(pthread\_t tid, void** retval) function performs a non-blocking join with the thread specified in the passed tid. wef the thread is busy then an error is returned. The tool check for this error and if the thread is busy, it will step\_and\_notify and wait before calling pthread\_tryjoin again. On the second call to pthread\_tryjoin, the thread should terminate. wef it still returns a busy status then it must be assumed that the schedule is invalid and an assertion failure should be raised.

Of course, one of the main disadvantages to this approach is that it reduces the portability of the tool. In its current state, pthread\_tryjoin\_np is a non-standard GNU extension and is thus non-portable. 

The positives of using the above approach outweigh the negatives, however, because it prevents the calling thread from blocking. wef the calling thread block on each call to the pthread\_join there might arise a situation in which all calling threads were blocked and thus a deadlock would occur.
\clearpage
\subsubsection{pthread\_mutex\_lock() and pthread\_mutex\_unlock()}
>>>>>>> .r38

<<<<<<< .mine
The main problem we encountered when using dynamic interception to implement the tool was with the LD\_PRELOAD environment variable. Unless otherwise specified, setting the LD\_PRELOAD variable will be operating system wide. So any calls by any programs to the Pthreads API will result in those calls being intercepted. Needless to say this can be disastrous for a user if not handled correctly, since this can cause the whole operating system to crash. To remedy this problem, we wrote the script to set the LD\_PRELOAD variable for only the executable which is is passed and to unset the LD\_PRELOAD variable as soon as execution finisheds. This prevent any unneccessary function interception and contains the intercepting to only the program the user wishes to test. 
=======
Below is the implementation for mutex locking and unlocking:
\lstinputlisting[float=h!,caption=wemplementation of mutex lock and unlock]{lockandunlock.c}
>>>>>>> .r38

<<<<<<< .mine
Another issue we found when using the LD\_PRELOAD was an issues with a program's permissions. In order to prevent a malicious user from intercepting system calls, a program must have the relevant permission to link and load a dynamic library. The GNU loader will check a program's permission on loading and determine if it is setuid or setgid. The loader will generally assume that a program does not have these permission and will thus greatly limit its permissions on loading. While this is not problematic when running programs from within the user's own profile, running programs on a remote server via ssh or on any machine where the owner of the program does not have priveleged acces will result in a segmentation fault on loadtime.  
=======
When a thread attempts to lock a mutex, it must first step\_and\_notify to move the schedule along and then wait to determine whether or not other threads are waiting on the mutex lock. 
>>>>>>> .r38

<<<<<<< .mine
There also a problem when using dlsym() to obtain a handle to function calls. In particular the problem arose when implementing pthread\_cond\_wait(). The dlsym() function makes an unversioned lookup and by default this lookup will match the oldest symbol in the dynamic shared library. wef any new features are added to a libary in the future then the dlsym() will still call the older version. Failing to use a versioned dlsym() call resulted in the older version of pthread\_cond\_wait being returned which returned a reference to a different pthread\_cond variable. wef the the initial pthread\_cond if located at 0x600d80, for example, then using the unversioned dlsym() function will result in the pthread\_cond variable begin returned at 0x7ffff00008c0. In this scenario, however, the main thread will still assume that the pthread\_cond variable is located at 0x600d80 and so will wait on this variable and receive no signal. To resolve this problem, it was neccessary to use a versioned dlysym() call to return the GLweBC\_2.3.2 version of the pthread\_cond\_wait() function.
=======
So that threads are no indefinitely blocked when waiting on a mutex, the tool makes use of the pthread\_mutex\_trylock function. When a thread calls this function it tries to lock the mutex. wef the mutex if already locked then an error occurs, otherwise the thread successfully locks the mutex. The errors which is usually returned in the EBUSY error value, which indicates that the mutex is already locked. 
>>>>>>> .r38

<<<<<<< .mine
\section{Pthreads API}
=======
The important feature of pthread\_mutex\_trylock is that it is a non-blocking call to lock the mutex, so the calling thread will continue execution if an EBUSY value is returned. wef, instead of using pthread\_mutex\_trylock, the tool used the standard mutex lock function: pthread\_mutex\_lock, threads might end up being indefinitely blocked waiting on the schedule to obtain the mutex lock, thus leading to a global deadlock. 
>>>>>>> .r38

<<<<<<< .mine
This project intercept calls made to thread creation, thread joining, mutex locking and unlocking and wait on condition variables. The various implementation details for each of these functions are discussed below.

\subsection{pthread\_create() and run\_thread()}

The source code for pthread\_create() using function intercepting is shown below:
\lstinputlisting[float=h!, caption=Intercepting pthread\_create]{create.c}

On the first call to pthread\_create() the tool will look for the location of the schedule in an environment variable and read the schedule file using this as its schedule. The tool will also check for the relevant flags and set these flags accordingly.

Each time a thread is created a params struct is instantiated to hold the relevant arguments for a thread. 

The params struct is shown below:
\lstinputlisting[float=h!, caption=The params struct]{params.c}

Params holds a function pointer to the function to be run in the created pthread, the instrumented ID of the thread and any arguments that need to be passed to the thread function.

Once a thread's parameters have been set up, a call to the real pthread\_create() function is made through the function pointer real\_create. This newly created thread is then told to run the run\_thread() function and is passed the params struct as its arguments. At this point in the execution, the params struct is fully instantiated so the original callee's function can be run through the run\_thread() function.

The run\_thread() function is shown below:
\lstinputlisting[float=h!, caption=The run\_thread function]{runthread.c}


The first thing the run\_thread() function does is insert the calling thread's ID into the Thread Map. The reason this is done in this function rather than on the call to pthread\_create() is because a call to create a pthread must be done by an existing thread and thus calls to pthread\_self() will return the pthread ID of the thread which called the create function not the ID of the thread which we are intending to create.

For example, consider the following simple scenario:
\begin{itemize}
\item The main thread with instrumented ID call pthread\_create()
\item pthread\_self() is called to obtain the ID of the thread to place in the Thread Map
\item Because the call to pthread\_self() was made by the main thread its pthread ID will be returned.
\item Thus there will be two duplicate entries for the pthread ID for the main thread with different instrumented IDs.
\end{itemize} 

For this reason, it far more desirable to insert the ID in the run\_thread() function.

Before a thread can be run it must wait on the schedule. Once it is eligible to be run the function passed in the params struct will be executed. A function might make other calls to Pthread API functions and these are handled accordingly. After a thread has finished its execution. wet must increase the schedule step and notify other waiting threads so that they can run.

By creating another thread to run the function that was passed to the initial call to pthread\_create(), we am able to force threads to follow the schedule. Without this approach, threads would have to be run within the call to pthread\_create() and this would cause threads to be forced to wait on each thread creation call, resulting in no proper thread scheduling. The thread created for run\_thread can be viewed as a worker threads, as it is designated to handle the thread scheduling without any other threads having to wait on its completion.

\subsection{pthread\_join()}
Below is the implementation for pthread\_join():

\lstinputlisting[float=h!, caption=Implementation for pthread\_join]{pthreadjoin.c}

When a call to pthread\_join(pthread\_t, void** retval) is made, the calling thread will wait for the thread specified in pthread\_t to terminate and any return values are stored in the retval. 

When a call to pthread\_join is made. The calling thread must increase the global step to move the schedule along and notify waiting threads. This is to ensure that other threads can run before the real call to pthread\_join is made, blocking the calling thread. The calling thread must then wait until it is scheduled to run. After waiting the calling thread calls the pthread\_tryjoin function on the ID of the threads it is trying to join. 

The pthread\_tryjoin\_np(pthread\_t tid, void** retval) function performs a non-blocking join with the thread specified in the passed tid. wef the thread is busy then an error is returned. The tool check for this error and if the thread is busy, it will step\_and\_notify and wait before calling pthread\_tryjoin again. On the second call to pthread\_tryjoin, the thread should terminate. wef it still returns a busy status then it must be assumed that the schedule is invalid and an assertion failure should be raised.

Of course, one of the main disadvantages to this approach is that it reduces the portability of the tool. In its current state, pthread\_tryjoin\_np is a non-standard GNU extension and is thus non-portable. 

The positives of using the above approach outweigh the negatives, however, because it prevents the calling thread from blocking. If the calling thread blocked on each call to the pthread\_join there might arise a situation in which all calling threads were blocked and thus a deadlock would occur.



\clearpage
\subsection{pthread\_mutex\_lock() and pthread\_mutex\_unlock()}

Below is the implementation for mutex locking and unlocking:
\lstinputlisting[float=h!,caption=Implementation of mutex lock and unlock]{lockandunlock.c}

When a thread attempts to lock a mutex, it must first step\_and\_notify to move the schedule along and then wait to determine whether or not other threads are waiting on the mutex lock. 

So that threads are no indefinitely blocked when waiting on a mutex, the tool makes use of the pthread\_mutex\_trylock function. When a thread calls this function it tries to lock the mutex. wef the mutex if already locked then an error occurs, otherwise the thread successfully locks the mutex. The errors which is usually returned in the EBUSY error value, which indicates that the mutex is already locked. 

The important feature of pthread\_mutex\_trylock is that it is a non-blocking call to lock the mutex, so the calling thread will continue execution if an EBUSY value is returned. wef, instead of using pthread\_mutex\_trylock, the tool used the standard mutex lock function: pthread\_mutex\_lock, threads might end up being indefinitely blocked waiting on the schedule to obtain the mutex lock, thus leading to a global deadlock. 

Unlocking a mutex requires a thread only to step\_and\_notify and wait. Once ready, a thread simply makes the call to the real mutex\_unlock through the dlsym() handle. The overridden function can then return the result of this unlock.


\subsection{pthread\_cond\_wait()}

Below is the implementation for pthread\_cond\_wait:

\lstinputlisting[float=h!, caption=Implementation of pthread\_cond\_wait]{condwait.c}

When a thread makes a call to pthread\_cond\_wait it always blocks, and on return it should have obtained the mutex lock it was originally trying to obtain in the call to pthread\_cond\_wait. 

In the intercepted version of pthread\_cond\_wait, a thread first calls step\_and\_notify and then makes a call to unlock the mutex. This is to ensure that it does enter a wait state with the mutex already locked. wef the calling thread did not unlock the mutex on entry to the function call and it already had the mutex locked then other threads would be forced to wait, potentially leading to a deadlock. A call to unlock a mutex that a thread does not already have ownership of will result in a failure with the EPERM result. This means that is it viable for a thread to attempt to unlock a mutex it does not already have locked with no adverse repercussions. 

Once a thread has obtained the mutex lock it can resume its processing and then step\_and\_notify before exiting the function call.



\clearpage
\subsection{pthread\_cond\_timedwait()}

The implementation for pthread\_cond\_timedwait() is shown below:
\lstinputlisting[float=h!, caption=Implementation of pthread\_cond\_timedwait]{timedwait.c}

We have implemented pthread\_cond\_timedwait by using the already overridden pthread\_mutex\_lock and pthread\_mutex\_unlock. The reason  for this is to ensure a synchronisation occurs before the thread obtains the lock and also when it releases the lock. We assume that if the thread does not obtain the lock with either of these two calls that it has timed out.

This approach limits the effectiveness of the timed wait function, since it will always reutnr ETIMEDOUT. For the sake of our implementation, however, it proved enough to lock and unlock the mutex and return ETIMEDOUT since for the programs we tested which made use of pthread\_cond\_timedwait ignored the return value anyway. Of course, any program that should use the return would not work with our tool but for the purposes of getting the tool to work in our testing environment this seemed appropriate.

\subsection{Problems}

One of the main problems we encountered when implementing the Pthread functions was in thread joining. There were numerous occasions when a thread could interrupt another thread before it had finished executing the run\_thread function and return EBUSY in the call to pthread\_tryjoin\_np. If any call to pthread\_tryjoin\_np returned EBUSY twice, then we would get an assertion failure and the tool would cease execution.




=======
Unlocking a mutex requires a thread only to step\_and\_notify and wait. Once ready, a thread simply makes the call to the real mutex\_unlock through the dlsym() handle. The overridden function can then return the result of this unlock.


\subsubsection{pthread\_cond\_wait()}

Below is the implementation for pthread\_cond\_wait:

\lstinputlisting[float=h!, caption=wemplementation of pthread\_cond\_wait]{condwait.c}

When a thread makes a call to pthread\_cond\_wait it always blocks, and on return it should have obtained the mutex lock it was originally trying to obtain in the call to pthread\_cond\_wait. 

In the intercepted version of pthread\_cond\_wait, a thread first calls step\_and\_notify and then makes a call to unlock the mutex. This is to ensure that it does enter a wait state with the mutex already locked. wef the calling thread did not unlock the mutex on entry to the function call and it already had the mutex locked then other threads would be forced to wait, potentially leading to a deadlock. A call to unlock a mutex that a thread does not already have ownership of will result in a failure with the EPERM result. This means that is it viable for a thread to attempt to unlock a mutex it does not already have locked with no adverse repercussions. 

Once a thread has obtained the mutex lock it can resume its processing and then step\_and\_notify before exiting the function call.

\subsection{Replay Mode}

The tool's replay mode is minimal and limited. wets purpose is to detect a given deadlock and then output the schedule that was followed leading to this deadlock. The tool's minimamlist replay mode was implemented using a simple monitor threads. On the first call to pthread\_create() in any program, the tool will create an initialise a thread to monitor the progress of thread execution. The monitor thread's aim is to check for satisfactory progress and, if satisfactory progress has not been made, to terminate program execution and print the schedule that the tool followed to reach the deadlocked state. 

My initial approach to the monitor thread is shown below:

\lstinputlisting[float=h!, caption=Initial Monitor Thread]{monitorbad.c}


The problem with this initial approach is that it will only check for deadlocks if the tool is passed an initial schedule. The other issue with this implementation is that the comparison between last\_step and the global step is almost redundant, since one we would assume that the step and last\_step are being continuously updated, therefore they will never be equal, so this comparison is rather pointless. To create a better monitor thread it was necessary to determine if the global step variable was still being implemented. 

we did this in the approach shown below:

\lstinputlisting[float=h!, caption=Modified Monitor Thread]{monitorgood.c}

As you can see this monitor will work independently of a schedule and will even work with the tool if no schedule is passed, since the global step is incremented each time step\_and\_notify is called and the tool builds the followed schedule each time step\_and\_notify is called.


>>>>>>> .r38
\chapter{Testing and Evaluation}

This section covers the different tests we used to test the overall effectiveness of the tool and to evaluate its performance under a variety of different scenarios.

\section{Testing Method}

In order to test the tool, we have taken three testing approaches. The first is running the tool alongside some hand-written tests with schedules generated by KLEEE-THREADS. The second approach was taking the existing test suites used by KLEE-THREADS to generate schedules and then run these existing programs with the tool. The third set of tests are to demonstrate the stability and efficiency of the tool and these are variety of hand-written scenarios.

\section{Hand-written Tests}

In order to test that the tool works as  expected, we wrote a number of simple hand-written tests to determine if the the tool will follow schedules faithfully.  In all of the tests the tool followed the schedules output by KLEE-THREADS. Under almost every scenario, KLEE-THREADS output a large amount of different schedules for any given executable.

<<<<<<< .mine
In order to determine in the passed schedule has been followed faithully by the tool, we have modified the tool to build an array of thread ID in the order in which they were executed. we then compare the build schedule with the original schedule that was passed and if they match then we know that the tool has followed the schedule faithfully. we altered the step\_and\_notify function to log the instrumented ID of the calling thread each time it is called. Since step\_and\_notify is called at every synchronisation point, it seemed suitable to use this approach to build the followed schedule. 
=======
In order to determine in the passed schedule has been followed faithully by the tool, we have modified the tool to build an array of thread weD in the order in which they were executed. we then compare the build schedule with the original schedule that was passed and if they match then we know that the tool has followed the schedule faithfully. we altered the step\_and\_notify function to log the instrumented weD of the calling thread each time it is called. Since step\_and\_notify is called at every synchronisation point, it seemed suitable to use this approach to build the followed schedule. 
>>>>>>> .r38

<<<<<<< .mine
All of the hand-written tests were run in the tool's FOLLOW\_AND\_TERMINATE mode so that the end of the schedule could be reached and then compared against the schedule the that was generated on execution. Testing the accuracy of the tool in the other modes cause problems. Testing in its default mode will result in the tool hanging and no further progress being made, thus it would be impossible to accurately replay the schedule the tool has followed, since there is no way to extract the followed schedule. 
=======
All of the hand-written tests were run in the tool's FOLLOW\_AND\_TERMINATE mode so that the end of the schedule could be reached and then compared against the schedule the that was generated on execution. Testing the accuracy of the tool in the other modes cause problems. Testing in its default mode will result in the tool hanging and no further progress being made, thus it would be impossible to accurately replay the schedule the tool has followed, since there is no way to extract the followed schedule. 
%%Testing in the FOLLOW\_AND\_RESUME mode (in which the tool follows a schedule to its end and then resumes normal execution) will result in a data race to verify the followed schedule, thus resulting in failure so even though the tool has followed the schedule accurately, it cannot be proved.
>>>>>>> .r38

To determine whether or not the tool has faithfully followed the schedule or not, at the end of a given execution the tool will compare the two schedules (the one it was passed and the one it generated) and throw an error code if the schedules do not match or else it will terminate normally. Using this method allows the use of a simple script to test if the tool has terminated successfuly.



To test the basic effectiveness of the tool, we tested it with a simple program that makes use of all of the Pthread API functions that we have overridden. The program we tested under KLEE-THREADS is shown below:
\clearpage
\lstinputlisting[float=h!, caption=Smple program using all overridden features]{condtest.c}

The way in which the program performs is fairly self-exlpanatory. The only thing of note is the assert(false) at the end of the program on line 52. The purpose of having this is to force KLEE-THREADS to generate a schedule which will find this assertion failure so that we can generate longer schedules to test the program, rather than shorter ones that only run to the first bug and no further.

KLEE-THREADS generated 131 schedules leading to bugs when run on this program. we did not run the tool with every schedule but instead opted to test it with a variety of schedules a number of times. In total, we tested 20 of the generated schedules 100 times each and examined the results. To determine which schedules we should use, we used a random number generator \footnote{http://www.random.org/} to generate random numbers between 1 and 131 and then tested the schedules with the matching numbers. This was to guarantee fairness to ensure that we could not deliberately fix the schedules to suit my tool.



The results are shown below:
\input{condtestresults.tex}

In all of the tests we ran the tool followed the schedule as expected. This, in itself, is not particularly surprising (or exciting) and the purpose of running these controlled tests was merely to demonstrate that the tool worked well in a predictable environment and that all of the different API calls had been implemented correctly.

\subsection{Deadlock Tests}

we also tested the tool with a program that leads to a deadlock. This test was written in conjunction with KLEE-THREADS and aims to force the program to deadlock. 
\clearpage
The program is shown below:

\lstinputlisting[float=h!, caption=A program leading to deadlock]{deadlocktest.c}
\clearpage
The program is more complicated and diverse than the previous one and was written to test the limits of the tool by generating longer schedules to see what errors occurred under each condition.

Running KLEE-THREADS with the following options results in 49 schedules being generated. 

Rather than using a random number generator to test the results, we opted to run tests on the first 10 schedules to check for any errors in both our recording process and the tool's execution order.

\input{deadlocktestres.tex}

As can be observed frpm the results, the tool performed well under harsher test conditions. With a longer schedule, the tool succesfully followed it until the end and with a more complex execution structure, the tool was able to maintain the integrity of its Thread Map and start and stop threads on demand.

These tests demonstrate that the tool is able to continue performing and providing deterministic replay with arbtitrarily long schedules. This is a critical feature of the tool, since more complex schedules will inevitably produce much large execution schedules and we would want the tool to perform under a multitude of conditions.

We can also observe from these results that the toll is accurately following the schedules it has been passed, rather than following some other thread ordering. It is essential that the tool follows its passed schedule accurately in order to provide deterministic replay. These results above demonstrate the tool's effectiveness.



\subsection{Testing RECORD mode}

To test the record mode, we wrote a simple program that might lead to a deadlock under certain scenarios. We then ran the program in the tool's record mode to record a schedule that led to a deadlock. Once we had generate this schedule, we could rerun the program with this schedule and see if it lead to a deadlock.If by running the tool with the generated schedule we recorded the same output, then we could assume that the tool's RECORD mode was working correctly.

To perform this test we used a version of the classic dining philosophers problem. 

\section{KLEE-THREADS Test Suite}
We wrote several of these tests, ran KLEE-THREADS on these programs and then ran the original programs with the tool.


\section{Efficiency Tests}

We have conducted efficiency tests to show how the tool performs under various situations, but also to demonstrate the minimal overhead that is incurred by using function interception. A system call is typically expensive for a program to make anyway, so intercepting these system calls and forcing certain schedules to be followed incurs a minimal additional overhead to the original system call. 

\subsection{Comparison of Wrapper Library and Function Intercepting Efficiency}

There appears to be no notable difference in efficienct between using a wrapper library or function intercepting to implement the tool.


<<<<<<< .mine
The version using the wrapper library and the version using function interception are both shown below:
\clearpage
\lstinputlisting[float=h!, caption=Wrapper Library Speed Test]{wrapperrace.c}
\clearpage
\lstinputlisting[float=h!, caption=Function Interception Speed Test]{interceptrace.c}
=======
The version using the wrapper library adn the version using function interception are both shown below:
\clearpage
\lstinputlisting[float=h!, caption=Wrapper Library Speed Test]{wrapperrace.c}
\clearpage
\lstinputlisting[float=h!, caption=Function Interception Speed Test]{interceptrace.c}
>>>>>>> .r38

<<<<<<< .mine
Since the version of the tool which uses function intercepting is set up using a python script, we set the environment variables manually for each execution so that there would be no overhead incurred by using the Python script. we/O also requires system calls so we executed both versions with no output other than the final value of GLOB.

We tested four schedules overall and looked for two things. First we tested the speed, averaged over ten executions, second we tested to check that each version of the tool output the same value for GLOB after each run.

We ran the tests in the tool's normal mode of execution. The schedules we passed the tool were handwritten. we did this to ensure that the programs terminated normally. The tests were only written for thread creation and joining but these two methods proved enough to test the difference in speed between the two. 

We ran the program with the same schedule [insert schedule here] 20 times with each implementation of the tool to see which version of the tool was faster. The results are shown below:

%%\input{wvsrres.tex}


\subsection{Existing Software Tests}


To demonstrate the tool's minimal overhead, we have taken programs which have demonstrated an increase in execution speed by using the Pthreads library and then generated schedules for these programs, timed them without using the tool and with using the tool and then compared the differences.


=======
Since the version of the tool which uses function intercepting is set up using a python script, we set the environment variables manually for each execution so that there would be no overhead incurred by using the Python script. we/O also requires system calls so we executed both versions with no output other than the final value of GLOB.

We tested four schedules overall and looked for two things. First we tested the speed, averaged over ten executions, second we tested to check that each version of the tool output the same value for GLOB after each run.

We ran the tests in the tool's normal mode of execution. The schedules we passed the tool were handwritten. we did this to ensure that the programs terminated normally. The tests were only written for thread creation and joining but these two methods proved enough to test the difference in speed between the two. 

The results of running the each version of the program are shown below. we have only shown the average excution speed for each test below. To see the full results see the Appendix.


\subsection{Existing Software Tests}


To demonstrate the tool's minimal overhead, we have taken programs which have demonstrated an increase in execution speed by using the pthreads library and then generated schedules for these programs, timed them without using the tool and with using the tool and then compared the differences.


>>>>>>> .r38
\section{Portability}

<<<<<<< .mine
Perhaps one of the main problems with the tool in its current state is portability. While the tool has been designed to work with the Pthreads API and thus theoretically with any POSIX compliant machine. This is not the case unfortunately. 
=======
Perhaps one of the main problems with the tool in its current state is portability. While the tool has been designed to work with the Pthreads API and thus theoretically with any POSweX compliant machine. This is not the case unfortunately. 

On a Mac, the main issues was is overwritten header files. The OS X operating system keeps a system implementation of the wait() function. This cause our wait() function to be ignore since wait() has already been defined.

On a BSD implementation (FreeBSD)


\section{Overall Problems}
How the problems we encountered effected the course of the project.
>>>>>>> .r38

On a Mac, one of the issues was that the OS X operating system keeps a system implementation of the wait() function. This caused our wait() function to be ignored since wait() has already been defined. We fixed this by simply redefining the wait() function to be my\_wait().

Another problem we faced on the Mac was in compilation with finding the location of <malloc.h>. To fix this all we needed to do alter our Makefile to specify the location of malloc.h

While these two issues were trivial to fix, we had further problems with compilation in pthread\_tryjoin\_np. As expected, this function is not portable and has no implementation for the OS X operating system.

The function dlvsym() is also not supported on the Mac operating system. Rewriting the tool to use dlsym() in the pthread\_cond\_wait caused the same problem we had when originally implementing the pthread\_cond\_wait function in section: 4.3.3. 


On a BSD implementation (FreeBSD)

\section{Overall Evaluation}
Overall evaluation of the tool, how it compares with other similar tools etc.


\chapter{Conclusion}
\section{Achievements}
\section{Future Work}
The most obvious extension to the tool is to continue implementing the rest of the Pthreads API. While it is a large API, we think implementing the whole API is feasible and would result in a robust and reliable tool that could be used with all concurrent C programs that use the Pthreads API to provide deterministic replay. In combination with KLEETHREADS, this would a create a full record/replay suite to concurrenct C programs and would prove an invaluable tool to developers developing concurrent software in C.

Another possible extension to the tool would be to give a more thorough execution trace to the bug. In the its current state, the tool does not really provide that in-depth a trace after termination to the bug. So as an added, extension it would be desirable to provide the user with a stack trace or clear path to the fault after the program's terminationc. Also when the tool runs in its default execution state, it simply hangs on once it reaches the end of a schedule. 




\bibliography{mybib}{}
\bibliographystyle{nar}
\end{document}
